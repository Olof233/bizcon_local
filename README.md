# bizCon: Business Conversation Evaluation Framework for LLMs

<div align="center">

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Tests](https://img.shields.io/badge/tests-passing-brightgreen.svg)](https://github.com/Olib-AI/bizcon/actions)
[![GitHub Issues](https://img.shields.io/github/issues/Olib-AI/bizcon)](https://github.com/Olib-AI/bizcon/issues)
[![GitHub Stars](https://img.shields.io/github/stars/Olib-AI/bizcon)](https://github.com/Olib-AI/bizcon/stargazers)

**A comprehensive open-source framework for benchmarking Large Language Models on business conversation capabilities**

[ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ“– Documentation](#-documentation) â€¢ [ğŸ“Š Sample Results](#-sample-results) â€¢ [ğŸ¤ Contributing](#-contributing) â€¢ [ğŸ’¬ Community](#-community)

</div>

---

## ğŸ“‹ Table of Contents

<details>
<summary><strong>ğŸ“– Click to view full navigation</strong></summary>

- [ğŸ¯ Overview](#-overview)
- [âœ¨ Key Features](#-key-features)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ“– Documentation](#-documentation)
- [ğŸ“Š Sample Results](#-sample-results)
- [ğŸ—ï¸ Advanced Usage](#ï¸-advanced-usage)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ§ª Testing & Validation](#-testing--validation)
- [ğŸ’¬ Community](#-community)
- [ğŸ“ˆ Roadmap](#-roadmap)

</details>

---

## ğŸ¯ Overview

bizCon is a specialized evaluation framework designed to benchmark Large Language Models (LLMs) on realistic business conversation scenarios. Unlike generic benchmarks, bizCon focuses on practical business use cases involving professional communication, tool integration, and domain-specific knowledge.

### Why bizCon?

- **Business-Focused**: Evaluates models on real-world business scenarios
- **Multi-Dimensional**: Assesses 5 key aspects of business communication
- **Tool Integration**: Tests models' ability to use business tools effectively
- **Comparative Analysis**: Benchmark multiple models side-by-side
- **Enterprise-Ready**: Professional reporting and analysis capabilities

## âœ¨ Key Features

### ğŸ­ **Diverse Business Scenarios**
- **Product Inquiries**: Enterprise software consultations
- **Technical Support**: Complex troubleshooting and API integration
- **Contract Negotiation**: SaaS agreements and enterprise deals
- **Appointment Scheduling**: Multi-stakeholder coordination
- **Compliance Inquiries**: Regulatory and data privacy questions
- **Implementation Planning**: Software deployment strategies
- **Service Complaints**: Customer service and dispute resolution
- **Multi-Department**: Cross-functional project coordination

### ğŸ“Š **Comprehensive Evaluation Metrics**
1. **Response Quality** (25%) - Factual accuracy and completeness
2. **Business Value** (25%) - Strategic insight and actionable recommendations
3. **Communication Style** (20%) - Professionalism and tone appropriateness
4. **Tool Usage** (20%) - Effective integration with business tools
5. **Performance** (10%) - Response time and efficiency

### ğŸ› ï¸ **Business Tool Ecosystem**
- Knowledge Base Search
- Product Catalog Lookup
- Pricing Calculator
- Appointment Scheduler
- Customer History Access
- Document Retrieval
- Order Management
- Support Ticket System

### ğŸ¤– **Multi-Model Support**

<table>
<tr>
<td><strong>ğŸ¤– OpenAI</strong></td>
<td><strong>ğŸ§  Anthropic</strong></td>
<td><strong>ğŸŒŸ Mistral AI</strong></td>
</tr>
<tr>
<td>â€¢ GPT-4<br>â€¢ GPT-3.5-turbo<br>â€¢ GPT-4-turbo</td>
<td>â€¢ Claude-3-opus<br>â€¢ Claude-3-sonnet<br>â€¢ Claude-3-haiku</td>
<td>â€¢ Mistral-large<br>â€¢ Mistral-medium<br>â€¢ Mistral-small</td>
</tr>
</table>

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/Olib-AI/bizcon.git
cd bizcon

# Install dependencies
pip install -e .
```

### Basic Usage

1. **Set up your API keys:**
```bash
export OPENAI_API_KEY="your-openai-key"
export ANTHROPIC_API_KEY="your-anthropic-key"
export MISTRAL_API_KEY="your-mistral-key"
```

2. **Run a quick test:**
```bash
# ğŸš€ Test without API keys (uses mock models)
python test_framework.py

# ğŸ§ª Run unit and integration tests
python -m pytest tests/

# ğŸ¤– Test with real models (requires API keys)
python test_with_real_models.py
```

3. **Run a benchmark:**
```bash
# ğŸ“Š Compare models on specific scenarios
python run.py --scenarios product_inquiry_001 support_001 --verbose

# ğŸƒ Run full benchmark with custom config
python run.py --config config/models.yaml --output results/

# ğŸ’» Using CLI interface directly  
bizcon run --config config/models.yaml --output results/
```

4. **Explore available options:**
```bash
# ğŸ“‹ List all available scenarios
python run.py --list-scenarios
# or: bizcon list-scenarios

# ğŸ¤– List supported models  
python run.py --list-models
# or: bizcon list-models
```

### Configuration

Customize your evaluation in `config/models.yaml`:

```yaml
models:
  - provider: openai
    name: gpt-4
    temperature: 0.7
    max_tokens: 2048
  - provider: anthropic
    name: claude-3-sonnet
    temperature: 0.7
    max_tokens: 2048
```

Adjust evaluation settings in `config/evaluation.yaml`:

```yaml
evaluation:
  parallel: true
  num_runs: 3
  evaluator_weights:
    response_quality: 0.25
    business_value: 0.25
    communication_style: 0.20
    tool_usage: 0.20
    performance: 0.10
```

## ğŸ“– Documentation

### Project Structure

```
bizcon/
â”œâ”€â”€ config/                 # Configuration files
â”‚   â”œâ”€â”€ models.yaml        # Model configurations
â”‚   â””â”€â”€ evaluation.yaml    # Evaluation settings
â”œâ”€â”€ core/                  # Core evaluation pipeline
â”‚   â”œâ”€â”€ pipeline.py        # Main evaluation orchestrator
â”‚   â””â”€â”€ runner.py          # Scenario execution engine
â”œâ”€â”€ models/                # LLM provider integrations
â”‚   â”œâ”€â”€ openai.py         # OpenAI client
â”‚   â”œâ”€â”€ anthropic.py      # Anthropic client
â”‚   â””â”€â”€ mistral.py        # Mistral AI client
â”œâ”€â”€ scenarios/             # Business conversation scenarios
â”‚   â”œâ”€â”€ product_inquiry.py
â”‚   â”œâ”€â”€ technical_support.py
â”‚   â””â”€â”€ contract_negotiation.py
â”œâ”€â”€ evaluators/            # Evaluation metrics
â”‚   â”œâ”€â”€ response_quality.py
â”‚   â”œâ”€â”€ business_value.py
â”‚   â””â”€â”€ communication_style.py
â”œâ”€â”€ tools/                 # Business tool implementations
â”‚   â”œâ”€â”€ knowledge_base.py
â”‚   â”œâ”€â”€ scheduler.py
â”‚   â””â”€â”€ product_catalog.py
â”œâ”€â”€ visualization/         # Report generation
â”‚   â”œâ”€â”€ charts.py
â”‚   â””â”€â”€ report.py
â””â”€â”€ data/                  # Sample business data
    â”œâ”€â”€ knowledge_base/
    â”œâ”€â”€ products/
    â””â”€â”€ pricing/
```

### Creating Custom Scenarios

```python
from scenarios.base import BusinessScenario

class CustomBusinessScenario(BusinessScenario):
    def __init__(self, scenario_id=None):
        super().__init__(
            scenario_id=scenario_id or "custom_001",
            name="Custom Business Scenario",
            description="Your custom scenario description",
            industry="technology",
            complexity="medium",
            tools_required=["knowledge_base", "scheduler"]
        )
    
    def _initialize_conversation(self):
        return [{
            "user_message": "Your initial customer message",
            "expected_tool_calls": [
                {"tool_id": "knowledge_base", "parameters": {"query": "example"}}
            ]
        }]
    
    def _initialize_ground_truth(self):
        return {
            "expected_facts": ["Key fact 1", "Key fact 2"],
            "business_objective": "Help customer achieve X",
            "expected_tone": "professional"
        }
```

### Adding Custom Evaluators

```python
from evaluators.base import BaseEvaluator

class CustomEvaluator(BaseEvaluator):
    def __init__(self, weight=1.0):
        super().__init__(name="Custom Evaluator", weight=weight)
    
    def evaluate(self, response, scenario, turn_index, conversation_history, tool_calls):
        # Your evaluation logic here
        score = self.calculate_score(response)
        return {
            "score": score,
            "explanation": "Detailed explanation of the score",
            "max_possible": 10.0
        }
```

## ğŸ“Š Sample Results

<details>
<summary><strong>ğŸ“ˆ Click to view sample benchmark results</strong></summary>

### Overall Model Performance
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model           â”‚ Overall â”‚ Response    â”‚ Business    â”‚ Communicationâ”‚ Tool Usage  â”‚ Performance â”‚
â”‚                 â”‚ Score   â”‚ Quality     â”‚ Value       â”‚ Style       â”‚             â”‚             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ gpt-4           â”‚ 8.2/10  â”‚ 8.5/10      â”‚ 8.1/10      â”‚ 9.0/10      â”‚ 7.8/10      â”‚ 8.0/10      â”‚
â”‚ claude-3-sonnet â”‚ 7.9/10  â”‚ 8.2/10      â”‚ 7.8/10      â”‚ 8.8/10      â”‚ 7.5/10      â”‚ 7.2/10      â”‚
â”‚ claude-3-haiku  â”‚ 7.1/10  â”‚ 7.3/10      â”‚ 6.9/10      â”‚ 8.0/10      â”‚ 6.8/10      â”‚ 8.5/10      â”‚
â”‚ gpt-3.5-turbo   â”‚ 6.8/10  â”‚ 6.5/10      â”‚ 6.2/10      â”‚ 7.5/10      â”‚ 6.0/10      â”‚ 7.8/10      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Success Rates by Category
- **GPT-4**: Response Quality (89%), Tool Usage (78%), Communication Style (90%)
- **Claude-3-Sonnet**: Response Quality (86%), Tool Usage (75%), Communication Style (88%)
- **Claude-3-Haiku**: Response Quality (73%), Tool Usage (68%), Communication Style (80%)

### Report Outputs
- **ğŸ“Š Interactive HTML Report**: Charts, breakdowns, and detailed analysis
- **ğŸ“ˆ CSV Data Export**: Raw scores for custom analysis and visualization
- **ğŸ“ Markdown Summary**: Professional reports for sharing and documentation
- **ğŸ¯ Success Rate Analysis**: Model performance across business scenarios

</details>

## ğŸ—ï¸ Advanced Usage

### Parallel Evaluation
```bash
# Run multiple scenarios in parallel
python run.py --scenarios product_inquiry_001 support_001 contract_001 --parallel

# Or using CLI directly
bizcon run --scenarios product_inquiry_001 support_001 --parallel
```

### Custom Model Parameters
```yaml
models:
  - provider: openai
    name: gpt-4
    temperature: 0.3
    max_tokens: 1024
    parameters:
      seed: 42
      top_p: 0.9
```

### Scenario Categories
```bash
# Run all product inquiry scenarios
python run.py --scenarios product_inquiry_*

# Run scenarios by complexity
python run.py --scenarios complex_*
```

## ğŸ¤ Contributing

We welcome contributions from the community! Here's how you can help:

### Ways to Contribute
- ğŸ› **Report Bugs**: Open an issue with detailed reproduction steps
- âœ¨ **Suggest Features**: Propose new scenarios, evaluators, or tools
- ğŸ“ **Improve Documentation**: Help make our docs clearer
- ğŸ”§ **Submit Code**: Fix bugs or add new features
- ğŸ§ª **Add Test Cases**: Improve our test coverage

### Development Setup
```bash
git clone https://github.com/Olib-AI/bizcon.git
cd bizcon
pip install -e .

# Run framework validation (no API keys needed)
python test_framework.py

# Run full test suite  
python -m pytest tests/
```

### Contribution Guidelines
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Add tests for new functionality
5. Run the test suite (`pytest`)
6. Commit your changes (`git commit -m 'Add amazing feature'`)
7. Push to the branch (`git push origin feature/amazing-feature`)
8. Open a Pull Request

## ğŸ§ª Testing & Validation

### ğŸ¯ Framework Validation Status

<div align="center">

| Component | Status | Coverage |
|-----------|--------|----------|
| **Unit Tests** | âœ… PASSED (12/12) | Evaluators, Scenarios, Tools |
| **Integration Tests** | âœ… PASSED | End-to-end Pipeline |
| **Framework Tests** | âœ… PASSED | Mock Model Validation |
| **Report Generation** | âœ… WORKING | HTML, Markdown, CSV |
| **CLI Functionality** | âœ… OPERATIONAL | All Commands Available |
| **Data Integrity** | âœ… VERIFIED | JSON Files Valid |

</div>

### Running Tests

<details>
<summary><strong>ğŸ§ª Click to view test commands</strong></summary>

```bash
# ğŸš€ Quick framework validation (no API keys required)
python test_framework.py

# ğŸ“Š Full test suite with detailed output
python -m pytest tests/ -v

# ğŸ” Test specific components
python -m pytest tests/unit/test_evaluators.py::TestResponseQualityEvaluator
python -m pytest tests/integration/test_pipeline.py

# ğŸ¯ Test with coverage report
python -m pytest tests/ --cov=./ --cov-report=html
```

**No API keys needed** for framework validation - uses MockModelClient for comprehensive testing.

</details>

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ’¬ Community

- **Website**: [www.olib.ai](https://www.olib.ai)
- **GitHub**: [github.com/Olib-AI](https://github.com/Olib-AI)
- **Issues**: [Report bugs or request features](https://github.com/Olib-AI/bizcon/issues)
- **Discussions**: [Join the conversation](https://github.com/Olib-AI/bizcon/discussions)

## ğŸ‘¥ Authors

**[Akram Hasan Sharkar](https://github.com/ibnbd)** - *Author & Lead Developer*  
**[Maya Msahal](https://github.com/Mayamsah)** - *Co-Author & Research Contributor*

*Developed at [Olib AI](https://www.olib.ai)*

## ğŸ“– Research Paper

A detailed research paper describing the methodology, evaluation framework, and empirical results of bizCon will be published on arXiv.org. The paper link will be available here upon publication.

*Citation format will be provided once the paper is published.*

## ğŸ™ Acknowledgments

- Built with â¤ï¸ by [Akram Hasan Sharkar](https://github.com/ibnbd) and [Maya Msahal](https://github.com/Mayamsah) at [Olib AI](https://www.olib.ai)
- Inspired by the need for better business-focused LLM evaluation
- Thanks to all contributors who help make this project better

## ğŸ“ˆ Roadmap

<details>
<summary><strong>ğŸš€ View upcoming features and release history</strong></summary>

### ğŸ”® Upcoming Features

| Feature | Priority | Status | ETA |
|---------|----------|--------|-----|
| ğŸŒ **More LLM Providers** (Cohere, Together AI) | High | Planning | Q2 2024 |
| ğŸ“Š **Advanced Visualization Dashboards** | High | In Progress | Q2 2024 |
| ğŸ­ **Industry-Specific Scenario Packs** | Medium | Planning | Q3 2024 |
| âš¡ **Real-time Evaluation APIs** | Medium | Researching | Q3 2024 |
| ğŸ”— **Custom Webhook Integrations** | Low | Backlog | Q4 2024 |
| ğŸŒ **Multi-language Support** | Low | Backlog | Q4 2024 |

### ğŸ“‹ Version History

- **v0.3.0** *(Current)*: Multi-provider support, tool integration, success rate differentiation
- **v0.2.0**: Added visualization and reporting capabilities
- **v0.1.0**: Initial release with core evaluation framework

</details>

---

<div align="center">

**Made with â¤ï¸ by [Akram Hasan Sharkar](https://github.com/ibnbd) & [Maya Msahal](https://github.com/Mayamsah) at [Olib AI](https://www.olib.ai)**

[â­ Star us on GitHub](https://github.com/Olib-AI/bizcon) â€¢ [ğŸ“– Read the Docs](https://github.com/Olib-AI/bizcon/wiki) â€¢ [ğŸ› Report Issues](https://github.com/Olib-AI/bizcon/issues)

</div>