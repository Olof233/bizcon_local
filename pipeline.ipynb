{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e71040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Main entry point for running bizCon benchmarks.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import yaml\n",
    "import json\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "current_path = sys.argv[0]\n",
    "\n",
    "# Add parent directory to path for importing\n",
    "sys.path.insert(0, str(Path(current_path).resolve().parent))\n",
    "\n",
    "# Import our modules\n",
    "from models import get_model_client, list_supported_models\n",
    "from scenarios import load_scenarios, list_available_scenarios\n",
    "from evaluators import get_all_evaluators\n",
    "from tools import get_default_tools\n",
    "from core.pipeline import EvaluationPipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f50797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path):\n",
    "    \"\"\"Load configuration from YAML file.\"\"\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        return yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b2c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scenarios_by_config(config, scenario_ids=None):\n",
    "    \"\"\"\n",
    "    Load scenarios based on configuration and optional specific IDs.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dictionary\n",
    "        scenario_ids: Optional list of specific scenario IDs to load\n",
    "        \n",
    "    Returns:\n",
    "        List of scenario instances\n",
    "    \"\"\"\n",
    "    if scenario_ids:\n",
    "        # Load specific scenarios by ID\n",
    "        return load_scenarios(scenario_ids)\n",
    "    \n",
    "    # Check for scenarios in config\n",
    "    config_scenarios = config.get('evaluation', {}).get('scenarios', [])\n",
    "    if config_scenarios:\n",
    "        return load_scenarios(config_scenarios)\n",
    "    \n",
    "    # Check for scenario categories in config\n",
    "    scenario_categories = config.get('evaluation', {}).get('scenario_categories', [])\n",
    "    if scenario_categories:\n",
    "        # Get all available scenarios\n",
    "        available_scenarios = list_available_scenarios()\n",
    "        scenario_ids = []\n",
    "        \n",
    "        for scenario_id, metadata in available_scenarios.items():\n",
    "            category = scenario_id.split('_')[0]\n",
    "            if category in scenario_categories:\n",
    "                scenario_ids.append(scenario_id)\n",
    "        \n",
    "        return load_scenarios(scenario_ids)\n",
    "    \n",
    "    # Default to all scenarios\n",
    "    return load_scenarios(list(list_available_scenarios().keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c327d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models_from_config(config):\n",
    "    \"\"\"\n",
    "    Load model clients from configuration.\n",
    "    \n",
    "    Args:\n",
    "        config: Dictionary with model configurations\n",
    "        \n",
    "    Returns:\n",
    "        List of model client instances\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    for model_config in config.get('models', []):\n",
    "        provider = model_config.get('provider')\n",
    "        model_name = model_config.get('name')\n",
    "        \n",
    "        # Get API key from environment or config\n",
    "        api_key = os.environ.get(f\"{provider.upper()}_API_KEY\", model_config.get('api_key'))\n",
    "        if not api_key:\n",
    "            api_key = True\n",
    "        \n",
    "        if api_key:\n",
    "        # Create model client\n",
    "            model = get_model_client(\n",
    "                provider=provider,\n",
    "                model_name=model_name,\n",
    "                api_key=api_key,\n",
    "                temperature=model_config.get('temperature', 0.7),\n",
    "                max_tokens=model_config.get('max_tokens', 1024),\n",
    "                **(model_config.get('parameters', {}))\n",
    "            )\n",
    "            models.append(model)\n",
    "            print(f\"Initialized model: {model}\")\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6f58a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(config_path, output_dir, scenario_ids=None, parallel=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Run benchmark evaluation.\n",
    "    \n",
    "    Args:\n",
    "        config_path: Path to configuration file\n",
    "        output_dir: Directory to save output\n",
    "        scenario_ids: Optional list of scenario IDs to run\n",
    "        parallel: Whether to run evaluations in parallel\n",
    "        verbose: Whether to display detailed progress\n",
    "    \"\"\"\n",
    "    # Load configuration\n",
    "    config = load_config(config_path)\n",
    "    \n",
    "    # Load models\n",
    "    models = load_models_from_config(config)\n",
    "    if not models:\n",
    "        print(\"Error: No models loaded. Check your API keys and configuration.\")\n",
    "        return\n",
    "    \n",
    "    # Load scenarios\n",
    "    scenarios = load_scenarios_by_config(config, scenario_ids)\n",
    "    if not scenarios:\n",
    "        print(\"Error: No scenarios loaded. Check your scenario IDs or configuration.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Loaded {len(scenarios)} scenarios for evaluation\")\n",
    "    \n",
    "    # Load evaluators with weights from config\n",
    "    evaluator_weights = config.get('evaluation', {}).get('evaluator_weights', {})\n",
    "    evaluators = get_all_evaluators(weights=evaluator_weights)\n",
    "    \n",
    "    # Load tools with error rates from config\n",
    "    tool_error_rates = config.get('evaluation', {}).get('tool_error_rates', {})\n",
    "    tools = get_default_tools()\n",
    "    for tool_id, tool in tools.items():\n",
    "        if tool_id in tool_error_rates:\n",
    "            tool.error_rate = tool_error_rates[tool_id]\n",
    "    \n",
    "    # Get number of runs from config\n",
    "    num_runs = config.get('evaluation', {}).get('num_runs', 1)\n",
    "    \n",
    "    # Set up pipeline\n",
    "    pipeline = EvaluationPipeline(\n",
    "        models=models,\n",
    "        scenarios=scenarios,\n",
    "        evaluators=evaluators,\n",
    "        tools=tools,\n",
    "        num_runs=num_runs,\n",
    "        parallel=parallel,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # Run evaluation\n",
    "    print(f\"Running benchmark with {len(models)} models on {len(scenarios)} scenarios...\")\n",
    "    results = pipeline.run()\n",
    "    \n",
    "    # Create timestamped output directory\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    result_dir = os.path.join(output_dir, f\"benchmark_{timestamp}\")\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "    \n",
    "    # Save raw results\n",
    "    results_file = os.path.join(result_dir, \"results.json\")\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {results_file}\")\n",
    "    \n",
    "    # Generate report\n",
    "    pipeline.generate_report(result_dir)\n",
    "    \n",
    "    print(f\"Report generated in {result_dir}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nSummary of Results:\")\n",
    "    for model_id, score in results[\"summary\"][\"overall_scores\"].items():\n",
    "        print(f\"  {model_id}: {score:.2f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c71aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description=\"Run bizCon benchmarks\")\n",
    "parser.add_argument(\"--config\", \"-c\", type=str, default=\"config/local_models.yaml\",\n",
    "                    help=\"Path to configuration file\")\n",
    "parser.add_argument(\"--output\", \"-o\", type=str, default=\"output\",\n",
    "                    help=\"Directory to save results\")\n",
    "parser.add_argument(\"--scenarios\", \"-s\", type=str, nargs=\"+\",\n",
    "                    help=\"Specific scenario IDs to run\")\n",
    "parser.add_argument(\"--parallel\", \"-p\", action=\"store_true\",\n",
    "                    help=\"Run evaluations in parallel\")\n",
    "parser.add_argument(\"--verbose\", \"-v\", action=\"store_true\",\n",
    "                    help=\"Display detailed progress\")\n",
    "parser.add_argument(\"--list-scenarios\", \"-l\", action=\"store_true\",\n",
    "                    help=\"List available scenarios and exit\")\n",
    "parser.add_argument(\"--list-models\", \"-m\", action=\"store_true\",\n",
    "                    help=\"List supported models and exit\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "\n",
    "\n",
    "# run_benchmark(\n",
    "#     config_path=args.config,\n",
    "#     output_dir=args.output,\n",
    "#     scenario_ids=args.scenarios,\n",
    "#     parallel=args.parallel,\n",
    "#     verbose=args.verbose\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3f6932",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(\"config/local_models.yaml\")\n",
    "    # Load models\n",
    "models = load_models_from_config(config)\n",
    "scenarios = load_scenarios_by_config(config)[:1]\n",
    "evaluator_weights = config.get('evaluation', {}).get('evaluator_weights', {})\n",
    "evaluators = get_all_evaluators(weights=evaluator_weights)\n",
    "\n",
    "# Load tools with error rates from config\n",
    "evcon = load_config(\"config/evaluation.yaml\")\n",
    "tool_error_rates = evcon.get('tool_error_rates', {})\n",
    "tools = get_default_tools()\n",
    "for tool_id, tool in tools.items():\n",
    "    if tool_id in tool_error_rates:\n",
    "        tool.error_rate = tool_error_rates[tool_id]\n",
    "\n",
    "# Get number of runs from config\n",
    "num_runs = evcon.get('num_runs', 1)\n",
    "\n",
    "# Set up pipeline\n",
    "pipeline = EvaluationPipeline(\n",
    "    models=models,\n",
    "    scenarios=scenarios,\n",
    "    evaluators=evaluators,\n",
    "    tools=tools,\n",
    "    num_runs=num_runs,\n",
    "    parallel=False,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "evaluation_tasks = []\n",
    "for scenario in pipeline.scenarios:\n",
    "    for model in pipeline.models:\n",
    "        for run_num in range(pipeline.num_runs):\n",
    "            evaluation_tasks.append((model, scenario, run_num))\n",
    "\n",
    "model, scenario, run_num = evaluation_tasks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1628835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from core.runner import ScenarioRunner\n",
    "runner = ScenarioRunner(\n",
    "    model=model,\n",
    "    scenario=scenario,\n",
    "    evaluators=pipeline.evaluators,\n",
    "    tools=pipeline.tools\n",
    ")\n",
    "\n",
    "\n",
    "tool_definitions = []\n",
    "for tool_id in runner.scenario.tools_required:\n",
    "    if tool_id in runner.tools:\n",
    "        tool_definitions.append(runner.tools[tool_id].get_definition())\n",
    "\n",
    "\n",
    "initial_message = runner.scenario.get_initial_message()\n",
    "runner.conversation_history.append(initial_message)\n",
    "\n",
    "response = runner.model.generate_response(\n",
    "    messages=runner.conversation_history,\n",
    "    tools=tool_definitions if tool_definitions else None\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78047586",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"Response from {runner.model.model_name}:\\n{response['content']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bizcon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
